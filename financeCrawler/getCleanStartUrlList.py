import scrapy

from financeCrawler.items import FinancecrawlerItem


def getCleanStartUrlList(filename):
    """
    Takes as input the name of the txt file generated by next
    script. In this file each line is the url of a blog post published
    in the period (1 January 2008 - 15 August 2014). The function returns
    a list of all the urls to be scraped.
    """
    myfile = open(filename, "r")
    urls = myfile.readlines()
    return [url.strip() for url in urls]


class BWSpider(scrapy.Spider):
    # name of the spider
    name = "busweek"
    # domains in which the spider can operate
    allowed_domains = ["businessweek.com"]
    # list of urls to be scraped
    urls = getCleanStartUrlList('businessweek.txt')
    start_urls = urls

    def parse(self, response):
        # the parse method is called by default on each url of the
        # start_urls list
        item = FinancecrawlerItem()
        # the date, keywords and body attributes are retrieved from
        # the response page using the XPath query language
        item['date'] = response.xpath('//meta[@content][@name="pub_date"]/@content').extract()
        item['keywords'] = response.xpath('//meta[@content][@name="keywords"]/@content').extract()
        item['body'] = response.xpath('//div[@id = "article_body"]/p/text()').extract()
        # the complete item filled with all its attributes
        yield item